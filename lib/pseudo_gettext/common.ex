defmodule PseudoGettext.Common do
  @moduledoc """
  Common functions for modules that implement pseudolocalization of text
  in several markup formats.
  From [Wikipedia](https://en.wikipedia.org/wiki/Pseudolocalization):

  > Pseudolocalization (or pseudo-localization) is a software testing method
  > used for testing internationalization aspects of software.
  > Instead of translating the text of the software into a foreign language,
  > as in the process of localization, the textual elements of an application
  > are replaced with an altered version of the original language.
  >
  > These specific alterations make the original words appear readable,
  > but include the most problematic characteristics of the world's languages:
  > varying length of text or characters, language direction,
  > fit into the interface and so on.
  """

  alias PseudoGettext.Common
  import NimbleParsec
  # Strings in other languages are assumed to be ~35% longer than in English
  @language_expansion 0.35
  # Pad the string with extra characters to simulate ~35% longer words
  @padding_characters "~"

  # Create a parsec that will distinguish words from everything else
  # so that we only expand the words and so that we handle punctuation
  # in a natural way.
  #
  # We prefer "word~." instead of "word.~" (that's why we won't split blindly on whitespace)

  non_word_characters = String.to_charlist("_.!?,;:Â«Â»\"`()[]{} \n\f\r")
  word_characters = Enum.map(non_word_characters, fn c -> {:not, c} end)

  maybe_empty_non_word_char_sequence = utf8_string(non_word_characters, min: 0)
  # right = utf8_string(non_word_characters, min: 0)
  word_char_sequence = utf8_string(word_characters, min: 1)

  head =
    unwrap_and_tag(maybe_empty_non_word_char_sequence, :non_word)
    |> tag(:chunk)

  word =
    unwrap_and_tag(word_char_sequence, :word)
    |> unwrap_and_tag(maybe_empty_non_word_char_sequence, :non_word)
    |> tag(:chunk)

  defparsecp(:chunk_parsec, head |> repeat(word))


  defp split_into_chunks(text) do
    {:ok, words, "", _, _, _} = chunk_parsec(text)
    words
  end

  @doc """
  Pseudolocalizes a word, which can include other non-word characters.
  Such characters are handled intelligently.
  ## Examples:
      iex> alias PseudoGettext.Common
      PseudoGettext.Common

      iex> Common.pseudolocalize_word("text") |> to_string()
      "Å¥Ãªáº‹Å¥~"

      iex> Common.pseudolocalize_word("text!") |> to_string()
      "Å¥Ãªáº‹Å¥~!"

      iex> Common.pseudolocalize_word("(text!)") |> to_string()
      "(Å¥Ãªáº‹Å¥~!)"

      iex> Common.pseudolocalize_word("") |> to_string()
      ""
  """
  def pseudolocalize_chunk({:chunk, parts}) do
    word = Keyword.get(parts, :word, nil)
    non_word = Keyword.get(parts, :non_word, "")

    localized_word =
      case word do
        nil ->
          ""

        text ->
          length = String.length(text)
          nr_of_extra_characters = floor(length * @language_expansion)
          extra_characters = String.duplicate(@padding_characters, nr_of_extra_characters)

          new_graphemes =
            for grapheme <- String.graphemes(text) do
              Common.convert_grapheme(grapheme)
            end

          [new_graphemes, extra_characters]
      end

    [localized_word, non_word]
  end

  def pseudolocalize_text(text) do
    original_words = split_into_chunks(text)
    Enum.map(original_words, &pseudolocalize_chunk/1)
  end

  @doc """
  Converts a single grapheme (not a unicode codepoint!) into a localized version.
  You probably don't want to use this directly unless you want to implement your
  custom pseudolocalization function that deals with things like HTML tags
  or other special markup.
  ## Examples
      iex> alias PseudoGettext.Common
      PseudoGettext.Common
      iex> Common.convert_grapheme("A") |> to_string()
      "Ã…"
      iex> Common.pseudolocalize_word("D") |> to_string()
      "Ä"
      iex> Common.pseudolocalize_word("f") |> to_string()
      "Æ’"
  """
  def convert_grapheme(g) do
    case g do
      # Upper case
      "A" -> "Ã…"
      "B" -> "Æ"
      "C" -> "ÄŠ"
      "D" -> "Ä"
      "E" -> "È„"
      "F" -> "á¸ž"
      "G" -> "Ä "
      "H" -> "Èž"
      "I" -> "Ä°"
      "J" -> "Ä´"
      "K" -> "Ç¨"
      "L" -> "Ä¹"
      "M" -> "á¹€"
      "N" -> "Ã‘"
      "O" -> "Ã’"
      "P" -> "Æ¤"
      "Q" -> "ê–"
      "R" -> "È’"
      "S" -> "È˜"
      "T" -> "Å¤"
      "U" -> "Ãœ"
      "V" -> "á¹¼"
      "W" -> "áº‚"
      "X" -> "áºŒ"
      "Y" -> "áºŽ"
      "Z" -> "Å½"
      # Lower case
      "a" -> "Ã "
      "b" -> "Æ€"
      "c" -> "Ä‹"
      "d" -> "Ä‘"
      "e" -> "Ãª"
      "f" -> "Æ’"
      "g" -> "ÄŸ"
      "h" -> "ÈŸ"
      "i" -> "Ä±"
      "j" -> "Ç°"
      "k" -> "Ç©"
      "l" -> "Äº"
      "m" -> "É±"
      "n" -> "Ã±"
      "o" -> "Ã¸"
      "p" -> "Æ¥"
      "q" -> "Ê "
      "r" -> "È“"
      "s" -> "Å¡"
      "t" -> "Å¥"
      "u" -> "Ã¼"
      "v" -> "á¹½"
      "w" -> "áº"
      "x" -> "áº‹"
      "y" -> "Ã¿"
      "z" -> "Åº"
      # Digits
      "0" -> "ðŸ˜"
      "1" -> "ðŸ™"
      "2" -> "ðŸš"
      "3" -> "ðŸ›"
      "4" -> "ðŸœ"
      "5" -> "ðŸ"
      "6" -> "ðŸž"
      "7" -> "ðŸŸ"
      "8" -> "ðŸ "
      "9" -> "ðŸ¡"
      # Other characters are returned as they are
      other -> other
    end
  end

  def surround_by_brackets(iolist) do
    ["âŸª", iolist, "âŸ«"]
  end

  def not_pseudolocalized?(c) do
    (c in ?a..?z) or (c in ?A..?Z) or (c in ?0..?9)
  end

  def validate_pseudolocalized(text) do
    unicode_characters = to_charlist(text)
    errors =
      for {char, index} <- Enum.with_index(unicode_characters, 0), not_pseudolocalized?(char) do
        {<< char :: utf8 >>, index}
      end

    case errors do
      [] -> :ok
      _ -> {:error, errors}
    end
  end
end
